{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a9deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swin_model import Swin_model\n",
    "from unet_model import Unet_model\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3454367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \n",
    "    swin_8687 = Swin_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/swin-again_no_back_1000hu_8655.pth\")\n",
    "    swin_8675 = Swin_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/swin-again_no_back_1000hu_8675.pth\")\n",
    "    swin_8655 = Swin_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/swin-again_no_back_1000hu_8687.pth\")\n",
    "    unet_8530 = Unet_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/unet_1000_hu_160_0853.pth\")\n",
    "    unet_8550 = Unet_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/unet_1000_hu_160_8550.pth\")\n",
    "    unet_8551 = Unet_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/unet_1000_hu_160_w_augmentations_8551.pth\")\n",
    "    \n",
    "    return swin_8687, swin_8675, swin_8655, unet_8530, unet_8550, unet_8551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d501f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_8687, swin_8675, swin_8655, unet_8530, unet_8550, unet_8551 = load_model()\n",
    "ensemble_weights = [88, 87, 86.55, 85.30, 85.50, 85.51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd61bda",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa2b49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd, \n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    AddChanneld,\n",
    "    ToTensord\n",
    "\n",
    ")\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"images\"]),\n",
    "        EnsureChannelFirstd(keys=[\"images\"]),\n",
    "        Orientationd(keys=[\"images\"], axcodes=\"LPS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"images\"], a_min=-1000, a_max=1000,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"images\"], source_key=\"images\"),\n",
    "        EnsureTyped(keys=[\"images\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def test_dataloader(path2input, test_transforms = test_transforms):\n",
    "    root_dir = path2input\n",
    "    test_files_path = sorted(glob.glob(os.path.join(root_dir, \"**/*.nii.gz\"), recursive = True))\n",
    "    test_data = [{\"images\": image_name } for image_name in test_files_path]\n",
    "    test_ds = Dataset(data = test_data, transform=test_transforms)\n",
    "    test_loader = DataLoader(test_ds, batch_size = 1, shuffle = False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6d489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9566a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_dataloader(\"./test_inputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6d2016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e34ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(loader, model, path2save,test_transforms, roi_size = (288, 288, 288), sw_batch_size = 8):\n",
    "    os.makedirs(path2save, exist_ok=True)\n",
    "    post_transforms = Compose([\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=test_transforms,\n",
    "            orig_keys=\"images\",\n",
    "            meta_keys=None,\n",
    "            orig_meta_keys=None,\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True),\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=path2save, output_postfix='seg', resample=False),\n",
    "    ])   \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data in loader:\n",
    "            test_inputs = test_data[\"images\"].to(device)\n",
    "            test_data[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
    "            test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290601a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.9 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-02 13:33:13,097 INFO image_writer.py:190 - writing: test_outputs/swin_8655_out/PA000005/PA000005_seg.nii.gz\n",
      "2022-08-02 13:46:40,340 INFO image_writer.py:190 - writing: test_outputs/swin_8655_out/PA000016/PA000016_seg.nii.gz\n",
      "2022-08-02 14:00:06,800 INFO image_writer.py:190 - writing: test_outputs/swin_8675_out/PA000005/PA000005_seg.nii.gz\n",
      "2022-08-02 14:13:27,728 INFO image_writer.py:190 - writing: test_outputs/swin_8675_out/PA000016/PA000016_seg.nii.gz\n",
      "2022-08-02 14:26:53,304 INFO image_writer.py:190 - writing: test_outputs/swin_8687_out/PA000005/PA000005_seg.nii.gz\n"
     ]
    }
   ],
   "source": [
    "## Unet\n",
    "# predict_and_save(loader, unet_8530, \"./test_outputs/unet_8530_out/\", test_transforms)\n",
    "# predict_and_save(loader, unet_8550, \"./test_outputs/unet_8550_out/\", test_transforms)\n",
    "# predict_and_save(loader, unet_8551, \"./test_outputs/unet_8551_out/\", test_transforms)\n",
    "\n",
    "#Swin\n",
    "%time\n",
    "predict_and_save(loader, swin_8655, \"./test_outputs/swin_8655_out/\", test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4)\n",
    "predict_and_save(loader, swin_8675, \"./test_outputs/swin_8675_out/\", test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4)\n",
    "predict_and_save(loader, swin_8687, \"./test_outputs/swin_8687_out/\", test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20896621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82361a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5c336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_and_save(loader, path2model, path2save,test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4):\n",
    "    print(\"model-loaded\")\n",
    "\n",
    "    os.makedirs(path2save, exist_ok=True)\n",
    "    post_transforms = Compose([\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=test_transforms,\n",
    "            orig_keys=\"images\",\n",
    "            meta_keys=None,\n",
    "            orig_meta_keys=None,\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True),\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=path2save, output_postfix='seg', resample=False),\n",
    "    ])   \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data in loader:\n",
    "            test_inputs = test_data[\"images\"].to(device)\n",
    "            test_data[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
    "            test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616a6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
