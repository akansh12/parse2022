{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swin_model import Swin_model\n",
    "from unet_model import Unet_model\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "import monai\n",
    "import numpy as np\n",
    "from monai.inferers import sliding_window_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3454367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    swin_8687 = Swin_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/swin-again_no_back_1000hu_8655.pth\")\n",
    "    swin_8675 = Swin_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/swin-again_no_back_1000hu_8675.pth\")\n",
    "    swin_8655 = Swin_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/swin-again_no_back_1000hu_8687.pth\")\n",
    "    unet_8530 = Unet_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/unet_1000_hu_160_0853.pth\")\n",
    "    unet_8550 = Unet_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/unet_1000_hu_160_8550.pth\")\n",
    "    unet_8551 = Unet_model(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/selected_models/unet_1000_hu_160_w_augmentations_8551.pth\")\n",
    "    \n",
    "    return swin_8687, swin_8675, swin_8655, unet_8530, unet_8550, unet_8551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d501f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_8687, swin_8675, swin_8655, unet_8530, unet_8550, unet_8551 = load_model()\n",
    "ensemble_weights = [88, 87, 86.55, 85.30, 85.50, 85.51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd61bda",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2b49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd, \n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    KeepLargestConnectedComponent,\n",
    "    AddChanneld,\n",
    "    ToTensord\n",
    "\n",
    ")\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"images\"]),\n",
    "        EnsureChannelFirstd(keys=[\"images\"]),\n",
    "        Orientationd(keys=[\"images\"], axcodes=\"LPS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"images\"], a_min=-1000, a_max=1000,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"images\"], source_key=\"images\"),\n",
    "        EnsureTyped(keys=[\"images\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def test_dataloader(path2input, test_transforms = test_transforms):\n",
    "    root_dir = path2input\n",
    "    test_files_path = sorted(glob.glob(os.path.join(root_dir, \"**/*.nii.gz\"), recursive = True))\n",
    "    test_data = [{\"images\": image_name } for image_name in test_files_path]\n",
    "    test_ds = Dataset(data = test_data, transform=test_transforms)\n",
    "    test_loader = DataLoader(test_ds, batch_size = 1, shuffle = False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9566a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_dataloader(\"/scratch/scratch6/akansh12/challenges/parse2022/docker/test_inputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638ffc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_transforms_1 = Compose([AsDiscreted(keys = \"pred\", argmax=True)])\n",
    "post_transforms_2 = Compose([\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=test_transforms,\n",
    "            orig_keys=\"images\",\n",
    "            meta_keys=None,\n",
    "            orig_meta_keys=None,\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "        ),\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"/scratch/scratch6/akansh12/challenges/parse2022/docker/test_outputs/\", output_postfix='seg', resample=False),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2732ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [unet_8530, unet_8550, unet_8551, swin_8687, swin_8675, swin_8655]\n",
    "weights = np.array([85.30, 85.50, 85.51, 88, 87, 86.55])\n",
    "weights = weights / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8086ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'post_transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30431/157756478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtest_data_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpost_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecollate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30431/157756478.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtest_data_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpost_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecollate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'post_transforms' is not defined"
     ]
    }
   ],
   "source": [
    "roi_size = (288, 288, 288)\n",
    "sw_batch_size = 8\n",
    "with torch.no_grad():\n",
    "    for test_data in loader:\n",
    "        test_data_out = 0\n",
    "        for e, (model,w) in enumerate(zip(models,weights)):\n",
    "            out = {}\n",
    "            test_inputs = test_data[\"images\"].to(device)\n",
    "            model = model.to(device)\n",
    "            if e > 2:\n",
    "                roi_size = (96, 96, 96)\n",
    "                sw_batch_size = 4\n",
    "            out[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
    "            out = [post_transforms_1(i) for i in decollate_batch(out)]\n",
    "            test_data_out = test_data_out + w*out[0]['pred'][0]\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        test_data_out = test_data_out > 0.5\n",
    "        test_data_out = test_data_out.unsqueeze(0).unsqueeze(0)\n",
    "        test_data['pred'] = test_data_out\n",
    "        test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7848dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93af82a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07cf00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fe296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa5462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e34ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(loader, model, path2save,test_transforms, roi_size = (288, 288, 288), sw_batch_size = 8):\n",
    "    os.makedirs(path2save, exist_ok=True)\n",
    "    post_transforms = Compose([\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=test_transforms,\n",
    "            orig_keys=\"images\",\n",
    "            meta_keys=None,\n",
    "            orig_meta_keys=None,\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True),\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=path2save, output_postfix='seg', resample=False),\n",
    "    ])   \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data in loader:\n",
    "            test_inputs = test_data[\"images\"].to(device)\n",
    "            test_data[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
    "            test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290601a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.9 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-02 13:33:13,097 INFO image_writer.py:190 - writing: test_outputs/swin_8655_out/PA000005/PA000005_seg.nii.gz\n",
      "2022-08-02 13:46:40,340 INFO image_writer.py:190 - writing: test_outputs/swin_8655_out/PA000016/PA000016_seg.nii.gz\n",
      "2022-08-02 14:00:06,800 INFO image_writer.py:190 - writing: test_outputs/swin_8675_out/PA000005/PA000005_seg.nii.gz\n",
      "2022-08-02 14:13:27,728 INFO image_writer.py:190 - writing: test_outputs/swin_8675_out/PA000016/PA000016_seg.nii.gz\n",
      "2022-08-02 14:26:53,304 INFO image_writer.py:190 - writing: test_outputs/swin_8687_out/PA000005/PA000005_seg.nii.gz\n"
     ]
    }
   ],
   "source": [
    "## Unet\n",
    "predict_and_save(loader, unet_8530, \"./test_outputs/unet_8530_out/\", test_transforms)\n",
    "predict_and_save(loader, unet_8550, \"./test_outputs/unet_8550_out/\", test_transforms)\n",
    "predict_and_save(loader, unet_8551, \"./test_outputs/unet_8551_out/\", test_transforms)\n",
    "\n",
    "#Swin\n",
    "predict_and_save(loader, swin_8655, \"./test_outputs/swin_8655_out/\", test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4)\n",
    "predict_and_save(loader, swin_8675, \"./test_outputs/swin_8675_out/\", test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4)\n",
    "predict_and_save(loader, swin_8687, \"./test_outputs/swin_8687_out/\", test_transforms, roi_size = (96, 96, 96), sw_batch_size = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5fdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57689a29",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c61b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d99e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"./test_outputs/**/*.nii.gz\", recursive=True):\n",
    "    subject_names.append(i.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4c7071c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PA000005_seg.nii.gz'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(subject_names)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14df920b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test_outputs/swin_8655_out/PA000005/PA000005_seg.nii.gz',\n",
       " './test_outputs/swin_8675_out/PA000005/PA000005_seg.nii.gz',\n",
       " './test_outputs/swin_8687_out/PA000005/PA000005_seg.nii.gz',\n",
       " './test_outputs/unet_8551_out/PA000005/PA000005_seg.nii.gz']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob.glob(f\"./test_outputs/**/{np.unique(subject_names)[0]}\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f0a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6c14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dab154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20896621",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed82361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[[[0.0040, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0010, 0.0020, 0.0035,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0100, 0.0075],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0050, 0.0000, 0.0010,  ..., 0.0000, 0.0035, 0.0075],\n",
       "            [0.0010, 0.0035, 0.0010,  ..., 0.0100, 0.0070, 0.0080]],\n",
       " \n",
       "           [[0.0050, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0055, 0.0060],\n",
       "            [0.0005, 0.0000, 0.0000,  ..., 0.0010, 0.0060, 0.0035],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0070, 0.0005]],\n",
       " \n",
       "           [[0.0065, 0.0000, 0.0000,  ..., 0.0035, 0.0020, 0.0010],\n",
       "            [0.0070, 0.0000, 0.0000,  ..., 0.0030, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            ...,\n",
       "            [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0005, 0.0000,  ..., 0.0060, 0.0050, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0010, 0.0000]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0.0000, 0.0000, 0.0105,  ..., 0.0060, 0.0075, 0.0025],\n",
       "            [0.0000, 0.0005, 0.0000,  ..., 0.0045, 0.0085, 0.0055],\n",
       "            [0.0000, 0.0040, 0.0000,  ..., 0.0050, 0.0050, 0.0030],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0045, 0.0020, 0.0000,  ..., 0.0025, 0.0050, 0.0045],\n",
       "            [0.0040, 0.0065, 0.0000,  ..., 0.0000, 0.0015, 0.0060]],\n",
       " \n",
       "           [[0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0090, 0.0045],\n",
       "            [0.0015, 0.0060, 0.0000,  ..., 0.0020, 0.0040, 0.0030],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0005, 0.0000],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0090, 0.0000],\n",
       "            [0.0070, 0.0045, 0.0000,  ..., 0.0050, 0.0080, 0.0025]],\n",
       " \n",
       "           [[0.0010, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0015],\n",
       "            [0.0000, 0.0000, 0.0005,  ..., 0.0005, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0035,  ..., 0.0010, 0.0005, 0.0000],\n",
       "            ...,\n",
       "            [0.0000, 0.0045, 0.0025,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],\n",
       "            [0.0050, 0.0000, 0.0000,  ..., 0.0055, 0.0070, 0.0000]]]]]),\n",
       " 'images_meta_dict': {'sizeof_hdr': tensor([348], dtype=torch.int32),\n",
       "  'extents': tensor([0], dtype=torch.int32),\n",
       "  'session_error': tensor([0], dtype=torch.int16),\n",
       "  'dim_info': tensor([0], dtype=torch.uint8),\n",
       "  'dim': tensor([[  3, 512, 512, 258,   1,   1,   1,   1]], dtype=torch.int16),\n",
       "  'intent_p1': tensor([0.]),\n",
       "  'intent_p2': tensor([0.]),\n",
       "  'intent_p3': tensor([0.]),\n",
       "  'intent_code': tensor([0], dtype=torch.int16),\n",
       "  'datatype': tensor([4], dtype=torch.int16),\n",
       "  'bitpix': tensor([16], dtype=torch.int16),\n",
       "  'slice_start': tensor([0], dtype=torch.int16),\n",
       "  'pixdim': tensor([[1.0000, 0.6465, 0.6465, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]),\n",
       "  'vox_offset': tensor([0.]),\n",
       "  'scl_slope': tensor([nan]),\n",
       "  'scl_inter': tensor([nan]),\n",
       "  'slice_end': tensor([0], dtype=torch.int16),\n",
       "  'slice_code': tensor([0], dtype=torch.uint8),\n",
       "  'xyzt_units': tensor([2], dtype=torch.uint8),\n",
       "  'cal_max': tensor([0.]),\n",
       "  'cal_min': tensor([0.]),\n",
       "  'slice_duration': tensor([0.]),\n",
       "  'toffset': tensor([0.]),\n",
       "  'glmax': tensor([0], dtype=torch.int32),\n",
       "  'glmin': tensor([0], dtype=torch.int32),\n",
       "  'qform_code': tensor([1], dtype=torch.int16),\n",
       "  'sform_code': tensor([1], dtype=torch.int16),\n",
       "  'quatern_b': tensor([0.]),\n",
       "  'quatern_c': tensor([0.]),\n",
       "  'quatern_d': tensor([1.]),\n",
       "  'qoffset_x': tensor([164.1768]),\n",
       "  'qoffset_y': tensor([311.6768]),\n",
       "  'qoffset_z': tensor([-684.9000]),\n",
       "  'srow_x': tensor([[ -0.6465,   0.0000,   0.0000, 164.1768]]),\n",
       "  'srow_y': tensor([[  0.0000,  -0.6465,   0.0000, 311.6768]]),\n",
       "  'srow_z': tensor([[   0.0000,    0.0000,    1.0000, -684.9000]]),\n",
       "  'affine': tensor([[[-6.4648e-01,  0.0000e+00,  0.0000e+00,  1.6418e+02],\n",
       "           [ 0.0000e+00, -6.4648e-01,  0.0000e+00,  3.1168e+02],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00, -6.8490e+02],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]]),\n",
       "  'original_affine': tensor([[[-6.4648e-01,  0.0000e+00,  0.0000e+00,  1.6418e+02],\n",
       "           [ 0.0000e+00, -6.4648e-01,  0.0000e+00,  3.1168e+02],\n",
       "           [ 0.0000e+00,  0.0000e+00,  1.0000e+00, -6.8490e+02],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       "         dtype=torch.float64),\n",
       "  'as_closest_canonical': tensor([False]),\n",
       "  'spatial_shape': tensor([[512, 512, 258]], dtype=torch.int16),\n",
       "  'original_channel_dim': ['no_channel'],\n",
       "  'filename_or_obj': ['test_inputs/PA000005/image/PA000005.nii.gz']},\n",
       " 'images_transforms': [{'class': ['Orientationd'],\n",
       "   'id': tensor([140002226284112]),\n",
       "   'orig_size': [tensor([512]), tensor([512]), tensor([258])],\n",
       "   'extra_info': {'meta_key': ['images_meta_dict'],\n",
       "    'old_affine': tensor([[[-6.4648e-01,  0.0000e+00,  0.0000e+00,  1.6418e+02],\n",
       "             [ 0.0000e+00, -6.4648e-01,  0.0000e+00,  3.1168e+02],\n",
       "             [ 0.0000e+00,  0.0000e+00,  1.0000e+00, -6.8490e+02],\n",
       "             [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]],\n",
       "           dtype=torch.float64)}},\n",
       "  {'class': ['CropForegroundd'],\n",
       "   'id': tensor([140002226284880]),\n",
       "   'orig_size': [tensor([512]), tensor([512]), tensor([258])],\n",
       "   'extra_info': {'box_start': tensor([[0, 0, 0]]),\n",
       "    'box_end': tensor([[512, 512, 258]])}},\n",
       "  {'class': ['EnsureTyped'],\n",
       "   'id': tensor([140002226285392]),\n",
       "   'orig_size': [tensor([512]), tensor([512]), tensor([258])]}],\n",
       " 'foreground_start_coord': tensor([[0, 0, 0]]),\n",
       " 'foreground_end_coord': tensor([[512, 512, 258]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5c336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616a6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
