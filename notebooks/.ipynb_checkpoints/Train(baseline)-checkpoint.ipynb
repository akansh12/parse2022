{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "181df7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torchio as tio\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06fae10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper\n",
    "def change_img_to_label_path(path):\n",
    "    parts = list(path.parts)\n",
    "    parts[parts.index(\"image\")] = \"label\"  \n",
    "    return Path(*parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6a988a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_root = Path(\"/scratch/scratch6/akansh12/Parse_data/processed_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "733cd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_paths_train = list(save_root.glob(\"train/*/image/*\"))\n",
    "subject_train = []\n",
    "\n",
    "for i in subjects_paths_train:\n",
    "    label_path = change_img_to_label_path(i)\n",
    "    subject = tio.Subject({\"CT\":tio.ScalarImage(i), \"Label\":tio.LabelMap(label_path)})\n",
    "    subject_train.append(subject)\n",
    "\n",
    "subjects_paths_val = list(save_root.glob(\"val/*/image/*\"))\n",
    "subject_val = []\n",
    "\n",
    "for i in subjects_paths_val:\n",
    "    label_path = change_img_to_label_path(i)\n",
    "    subject = tio.Subject({\"CT\":tio.ScalarImage(i), \"Label\":tio.LabelMap(label_path)})\n",
    "    subject_val.append(subject)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b815581",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = tio.Compose([\n",
    "            tio.CropOrPad((256, 256, 200)),\n",
    "            tio.RescaleIntensity((-1, 1))\n",
    "            ])\n",
    "\n",
    "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
    "\n",
    "\n",
    "val_transform = process\n",
    "train_transform = tio.Compose([process, augmentation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eacc9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tio.SubjectsDataset(subject_train, transform=train_transform)\n",
    "val_dataset = tio.SubjectsDataset(subject_val, transform=val_transform)\n",
    "\n",
    "sampler = tio.data.UniformSampler(patch_size=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00754c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches_queue = tio.Queue(\n",
    "     train_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n",
    "val_patches_queue = tio.Queue(\n",
    "     val_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31500ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_patches_queue, batch_size=batch_size, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_patches_queue, batch_size=batch_size, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "530d028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.float()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.float()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def log_images(self, img, pred, mask, name):\n",
    "        \n",
    "        results = []\n",
    "        pred = torch.argmax(pred, 1) # Take the output with the highest value\n",
    "        axial_slice = 50  # Always plot slice 50 of the 96 slices\n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][:,:,axial_slice]==0, mask[0][:,:,axial_slice])\n",
    "        axis[0].imshow(mask_[0], alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\")\n",
    "        \n",
    "        axis[1].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][:,:,axial_slice]==0, pred[0][:,:,axial_slice])\n",
    "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Pred\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ab82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf3eaf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = 0 #TODO\n",
    "trainer = pl.Trainer(gpus=gpus, logger=TensorBoardLogger(save_dir=\"/scratch/scratch6/akansh12/challenges/parse2022/notebooks/logs/\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     max_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d2a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | model   | UNet              | 5.8 M \n",
      "1 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.344    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scratch6/akansh12/env/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/scratch/scratch6/akansh12/env/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/scratch/scratch6/akansh12/env/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3336790ff41f48ddbf582c86f23809b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/scratch6/akansh12/env/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:686: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6642f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b73a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322425d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42998be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd706af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c80e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
