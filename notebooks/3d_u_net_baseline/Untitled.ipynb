{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c4384a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cfb9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import monai\n",
    "\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd, \n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd\n",
    ")\n",
    "\n",
    "# from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624294c",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e947aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/scratch/scratch6/akansh12/Parse_data/train/train/\"\n",
    "train_images = sorted(glob.glob(os.path.join(root_dir, \"*\", 'image', \"*.nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(root_dir, \"*\", 'label', \"*.nii.gz\")))\n",
    "\n",
    "data_dicts = [{\"images\": images_name, \"labels\": label_name} for images_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "set_determinism(seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba453ee",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f31401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "     LoadImaged(keys=['images', 'labels']),\n",
    "     EnsureChannelFirstd(keys = [\"images\", \"labels\"]),\n",
    "     Orientationd(keys=['images', 'labels'], axcodes = 'LPS'),\n",
    "     Spacingd(keys=['images', 'labels'], pixdim = (1.5,1.5,2), mode = (\"bilinear\", 'nearest')),\n",
    "     ScaleIntensityRanged(\n",
    "            keys=[\"images\"], a_min=-700, a_max=300,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "     CropForegroundd(keys=['images', 'labels'], source_key=\"images\"),\n",
    "     RandCropByPosNegLabeld(\n",
    "            keys=['images', 'labels'],\n",
    "            label_key=\"labels\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"images\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "     EnsureTyped(keys=['images', 'labels']),     \n",
    "          \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"images\", \"labels\"]),\n",
    "        EnsureChannelFirstd(keys=[\"images\", \"labels\"]),\n",
    "        Orientationd(keys=[\"images\", \"labels\"], axcodes=\"LPS\"),\n",
    "        Spacingd(keys=[\"images\", \"labels\"], pixdim=(\n",
    "            1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"images\"], a_min=-700, a_max=300,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"images\", \"labels\"], source_key=\"images\"),\n",
    "        EnsureTyped(keys=[\"images\", \"labels\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0306ef",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "572e4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [04:26<00:00,  2.93s/it]\n",
      "Loading dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:42<00:00,  4.71s/it]\n"
     ]
    }
   ],
   "source": [
    "train_ds = CacheDataset(\n",
    "    data = train_files, transform = train_transforms,\n",
    "    cache_rate = 1.0, num_workers = 4\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size = 2, shuffle = True, num_workers=2)\n",
    "val_ds = CacheDataset(\n",
    "    data = val_files, transform = val_transforms,\n",
    "    cache_rate = 1.0, num_workers = 4\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size = 1, shuffle = False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11727e",
   "metadata": {},
   "source": [
    "### Model defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edd1523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "\n",
    "UNet_meatdata = dict(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH\n",
    ")\n",
    "model = UNet(**UNet_meatdata).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ff481",
   "metadata": {},
   "source": [
    "### Loss function and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a37c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "loss_type = \"DiceLoss\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3935980",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer_metadata = {}\n",
    "for ind, param_group in enumerate(optimizer.param_groups):\n",
    "    optim_meta_keys = list(param_group.keys())\n",
    "    Optimizer_metadata[f'param_group_{ind}'] = {key: value for (key, value) in param_group.items() if 'params' not in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddb26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e148483ec24b44b2a5d7422c8855b4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/45, train_loss: 0.6867\n",
      "2/45, train_loss: 0.6869\n",
      "3/45, train_loss: 0.6846\n",
      "4/45, train_loss: 0.6794\n",
      "5/45, train_loss: 0.6792\n",
      "6/45, train_loss: 0.6813\n",
      "7/45, train_loss: 0.6818\n",
      "8/45, train_loss: 0.6701\n",
      "9/45, train_loss: 0.6757\n",
      "10/45, train_loss: 0.6768\n",
      "11/45, train_loss: 0.6724\n",
      "12/45, train_loss: 0.6710\n",
      "13/45, train_loss: 0.6733\n",
      "14/45, train_loss: 0.6716\n",
      "15/45, train_loss: 0.6678\n",
      "16/45, train_loss: 0.6688\n",
      "17/45, train_loss: 0.6686\n",
      "18/45, train_loss: 0.6642\n",
      "19/45, train_loss: 0.6686\n",
      "20/45, train_loss: 0.6621\n",
      "21/45, train_loss: 0.6631\n",
      "22/45, train_loss: 0.6591\n",
      "23/45, train_loss: 0.6569\n",
      "24/45, train_loss: 0.6623\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 600\n",
    "val_interval = 10\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "\n",
    "\n",
    "slice_to_track = 80\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data['images'].to(device),\n",
    "            batch_data['labels'].to(device)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss +=loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"train_loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, val_data in enumerate(val_loader):\n",
    "\n",
    "                val_inputs, val_labels = val_data['images'].to(device), val_data['labels'].to(device)\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(\n",
    "                            val_inputs, roi_size, sw_batch_size, model)\n",
    "\n",
    "                output = torch.argmax(val_outputs, dim=1)[0, :, :, slice_to_track].float()\n",
    "\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                dice_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    root_dir, \"/scratch/scratch6/akansh12/challenges/parse2022/temp/best_metric_model.pth\"))\n",
    "\n",
    "                best_model_log_message = f\"saved new best metric model at the {epoch+1}th epoch\"\n",
    "                print(best_model_log_message)\n",
    "\n",
    "                message1 = f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                message2 = f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                message3 = f\"at epoch: {best_metric_epoch}\"\n",
    "                print(message1, message2, message3)\n",
    "\n",
    "np.save(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/epoch_loss.npy\", epoch_loss_values)\n",
    "np.save(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/metric_values.npy\", metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d44b3c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b64d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448fd5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
