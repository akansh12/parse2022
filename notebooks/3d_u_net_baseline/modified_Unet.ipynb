{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a06c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd, \n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    "    Invertd,\n",
    "    RandGridDistortiond,\n",
    "    Rand3DElasticd,\n",
    "    RandRotate90d,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    RandZoomd,\n",
    "    CropForegroundd,\n",
    "    RandGaussianNoised,\n",
    "    ShiftIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandKSpaceSpikeNoised,\n",
    "    KeepLargestConnectedComponentd,\n",
    "    OneOf,\n",
    "    AddChanneld,\n",
    "    ToTensord\n",
    ")\n",
    "\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "from monai.visualize import blend_images, matshow3d, plot_2d_or_3d_image\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a6ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/scratch/scratch6/akansh12/Parse_data/train/train/\"\n",
    "train_images = sorted(glob.glob(os.path.join(root_dir, \"*\", 'image', \"*.nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(root_dir, \"*\", 'label', \"*.nii.gz\")))\n",
    "\n",
    "data_dicts = [{\"images\": images_name, \"labels\": label_name} for images_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a7d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"images\", \"labels\"]),\n",
    "#         AddChanneld(keys=[\"images\", \"labels\"]),\n",
    "#         Orientationd(keys=[\"images\", \"labels\"], axcodes=\"LPS\"),\n",
    "#         ScaleIntensityRanged(\n",
    "#             keys=[\"images\"],\n",
    "#             a_min=-1000,\n",
    "#             a_max=1000,\n",
    "#             b_min=0.0,\n",
    "#             b_max=1.0,\n",
    "#             clip=True,\n",
    "#         ),\n",
    "#         CropForegroundd(keys=[\"images\", \"labels\"], source_key=\"images\"),\n",
    "#         RandCropByPosNegLabeld(\n",
    "#             keys=[\"images\", \"labels\"],\n",
    "#             label_key=\"labels\",\n",
    "#             spatial_size=(128, 128, 128),\n",
    "#             pos=1,\n",
    "#             neg=1,\n",
    "#             num_samples=4,\n",
    "#             image_key=\"images\",\n",
    "#             image_threshold=0,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"images\", \"labels\"],\n",
    "#             spatial_axis=[0],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"images\", \"labels\"],\n",
    "#             spatial_axis=[1],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandFlipd(\n",
    "#             keys=[\"images\", \"labels\"],\n",
    "#             spatial_axis=[2],\n",
    "#             prob=0.20,\n",
    "#         ),\n",
    "#         RandRotate90d(\n",
    "#             keys=[\"images\", \"labels\"],\n",
    "#             prob=0.10,\n",
    "#             max_k=3,\n",
    "#         ),\n",
    "#         RandShiftIntensityd(\n",
    "#             keys=[\"images\"],\n",
    "#             offsets=0.10,\n",
    "#             prob=0.50,\n",
    "#         ),\n",
    "#         ToTensord(keys=[\"images\", \"labels\"]),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# val_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"images\", \"labels\"]),\n",
    "#         EnsureChannelFirstd(keys=[\"images\", \"labels\"]),\n",
    "#         Orientationd(keys=[\"images\", \"labels\"], axcodes=\"LPS\"),\n",
    "#         ScaleIntensityRanged(\n",
    "#             keys=[\"images\"], a_min=-1000, a_max=1000,\n",
    "#             b_min=0.0, b_max=1.0, clip=True,\n",
    "#         ),\n",
    "#         CropForegroundd(keys=[\"images\", \"labels\"], source_key=\"images\"),\n",
    "#         EnsureTyped(keys=[\"images\", \"labels\"]),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb5d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "     LoadImaged(keys=['images', 'labels']),\n",
    "     EnsureChannelFirstd(keys = [\"images\", \"labels\"]),\n",
    "     Orientationd(keys=['images', 'labels'], axcodes = 'LPS'),\n",
    "#      Spacingd(keys=['images', 'labels'], pixdim = (1.5,1.5,2), mode = (\"bilinear\", 'nearest')),\n",
    "     ScaleIntensityRanged(\n",
    "            keys=[\"images\"], a_min=-1000, a_max=1000,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "     CropForegroundd(keys=['images', 'labels'], source_key=\"images\"),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=['images', 'labels'],\n",
    "            label_key=\"labels\",\n",
    "            spatial_size=(128, 128, 128),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"images\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        EnsureTyped(keys=['images', 'labels']),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"images\", \"labels\"]),\n",
    "        EnsureChannelFirstd(keys=[\"images\", \"labels\"]),\n",
    "        Orientationd(keys=[\"images\", \"labels\"], axcodes=\"LPS\"),\n",
    "#         Spacingd(keys=[\"images\", \"labels\"], pixdim=(\n",
    "#             1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"images\"], a_min=-1000, a_max=1000,\n",
    "            b_min=0.0, b_max=1.0, clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"images\", \"labels\"], source_key=\"images\"),\n",
    "        EnsureTyped(keys=[\"images\", \"labels\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf826fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:  81%|███████████████████████████████████████████████████████████████████████████████████████████▉                     | 74/91 [02:58<00:39,  2.32s/it]"
     ]
    }
   ],
   "source": [
    "train_ds = CacheDataset(\n",
    "    data = train_files, transform = train_transforms,\n",
    "    cache_rate = 1.0, num_workers = 4\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size = 2, shuffle = True, num_workers=4)\n",
    "val_ds = CacheDataset(\n",
    "    data = val_files, transform = val_transforms,\n",
    "    cache_rate = 1.0, num_workers = 4\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size = 1, shuffle = False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37feec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "UNet_meatdata = dict(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH\n",
    ")\n",
    "model = UNet(**UNet_meatdata).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "loss_type = \"DiceLoss\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer_metadata = {}\n",
    "for ind, param_group in enumerate(optimizer.param_groups):\n",
    "    optim_meta_keys = list(param_group.keys())\n",
    "    Optimizer_metadata[f'param_group_{ind}'] = {key: value for (key, value) in param_group.items() if 'params' not in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ba56e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/best_metric_model_Unet_1000_hu.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0748ce12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15cde4bf6dd4d538e36b4fe1a75b30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4, train_loss: 0.6544\n",
      "2/4, train_loss: 0.6469\n",
      "3/4, train_loss: 0.6669\n",
      "4/4, train_loss: 0.6427\n",
      "5/4, train_loss: 0.6520\n",
      "epoch 1 average loss: 0.6526\n",
      "1/4, train_loss: 0.6335\n",
      "2/4, train_loss: 0.6316\n",
      "3/4, train_loss: 0.6388\n",
      "4/4, train_loss: 0.6478\n",
      "5/4, train_loss: 0.6447\n",
      "epoch 2 average loss: 0.6393\n",
      "1/4, train_loss: 0.6421\n",
      "2/4, train_loss: 0.6539\n",
      "3/4, train_loss: 0.6479\n",
      "4/4, train_loss: 0.6307\n",
      "5/4, train_loss: 0.6319\n",
      "epoch 3 average loss: 0.6413\n",
      "1/4, train_loss: 0.6427\n",
      "2/4, train_loss: 0.6473\n",
      "3/4, train_loss: 0.6366\n",
      "4/4, train_loss: 0.6395\n",
      "5/4, train_loss: 0.6336\n",
      "epoch 4 average loss: 0.6399\n",
      "1/4, train_loss: 0.6235\n",
      "2/4, train_loss: 0.6577\n",
      "3/4, train_loss: 0.6241\n",
      "4/4, train_loss: 0.6335\n",
      "5/4, train_loss: 0.6106\n",
      "epoch 5 average loss: 0.6299\n",
      "1/4, train_loss: 0.6323\n",
      "2/4, train_loss: 0.6183\n",
      "3/4, train_loss: 0.6394\n",
      "4/4, train_loss: 0.6129\n",
      "5/4, train_loss: 0.6407\n",
      "epoch 6 average loss: 0.6287\n",
      "1/4, train_loss: 0.6293\n",
      "2/4, train_loss: 0.6111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13924/2162856063.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/scratch6/akansh12/env/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/scratch6/akansh12/env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 600\n",
    "val_interval = 10\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([EnsureType(), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([EnsureType(), AsDiscrete(to_onehot=2)])\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data['images'].to(device),\n",
    "            batch_data['labels'].to(device)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss +=loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \"\n",
    "                f\"train_loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, val_data in enumerate(val_loader):\n",
    "\n",
    "                val_inputs, val_labels = val_data['images'].to(device), val_data['labels'].to(device)\n",
    "                roi_size = (256, 256, 256)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(\n",
    "                            val_inputs, roi_size, sw_batch_size, model)\n",
    "\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    root_dir, \"/scratch/scratch6/akansh12/challenges/parse2022/temp/Unet_3d_no_spacing_128.pth\"))\n",
    "\n",
    "                best_model_log_message = f\"saved new best metric model at the {epoch+1}th epoch\"\n",
    "                print(best_model_log_message)\n",
    "\n",
    "                message1 = f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                message2 = f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                message3 = f\"at epoch: {best_metric_epoch}\"\n",
    "                print(message1, message2, message3)\n",
    "\n",
    "np.save(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/epoch_loss_Unet_3d_no_spacing_128.npy\", epoch_loss_values)\n",
    "np.save(\"/scratch/scratch6/akansh12/challenges/parse2022/temp/metric_values_Unet_3d_no_spacing_128.npy\", metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ed97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
